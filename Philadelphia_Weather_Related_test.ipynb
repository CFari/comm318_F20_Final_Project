{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Philadelphia Weather Related",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u3MeMRPd9zZ"
      },
      "source": [
        "# General Imports, Settings, and Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRoz_kTmsbi1"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import datetime\n",
        "import math\n",
        "from google.cloud import bigquery\n",
        "import scipy.stats  as stats\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13GilbW6gXdb"
      },
      "source": [
        "## Formatting and the like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCIcA0l1gaym"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import matplotlib.ticker as mtick\n",
        "import matplotlib\n",
        "from decimal import Decimal\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "style.use('ggplot')\n",
        "# plt.tight_layout()\n",
        "# color_map = plt.get_cmap('tab20b')\n",
        "# colors = [  plt.cm.tab20b(x) for x in [0, 4, 8, 12, 16, 1, 5, 9, 13, 17, 2, 6, 10, 14, 18] ]\n",
        "colors = [ matplotlib.colors.to_hex(plt.cm.Dark2(x)) for x in [0,1,2,3,4,5,6,7,0,1,2,3,4,5,6,7,0,1,2,3,4,5,6,7,0,1,2,3,4,5,6,7,0,1,2,3,4,5,6,7,0,1,2,3,4,5,6,7,0,1,2,3,4,5,6,7,0,1,2,3,4,5,6,7,0,1,2,3,4,5,6,7,0,1,2,3,4,5,6,7]]\n",
        "plt.rcParams['axes.facecolor']='#ebeae8'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9DE4pofQeWN"
      },
      "source": [
        "## Helper methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj-WAWODOmQ6"
      },
      "source": [
        "def get_google_sheet_df(sheet_id: str, sheet_name: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Get data from a Google Sheet document.\n",
        "    \"\"\"\n",
        "\n",
        "    url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={requests.utils.quote(sheet_name)}\"\n",
        "    print(f\"Getting: {url}\")\n",
        "    return pd.read_csv(url).dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkdfkB82DPem"
      },
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def printable_p(p: float) -> str:\n",
        "    \"\"\"\n",
        "    Return a more human readable p-value.\n",
        "    \"\"\"\n",
        "    if p <= 0.006:\n",
        "        return f\"{p:0.1e}\"\n",
        "    return f\"{p:0.2f}\"\n",
        "\n",
        "def human_col_info(col_name: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Return more human-redable representations of columns if they exist.\n",
        "    \"\"\"\n",
        "    units = col_units[col_name] if col_name in col_units else r'$Unkn$'\n",
        "    name = col_names[col_name] if col_name in col_names else col_name\n",
        "    return name, units\n",
        "\n",
        "def normalize_to_mean(col: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Return a column (as a pandas.Series) with values \"normalized\" with a\n",
        "    mean of 0.0 and a standard deviation of 1.0\n",
        "    \"\"\"\n",
        "    return (col - col.mean()) / col.std()\n",
        "\n",
        "def plot_two_ts(df: pd.DataFrame, col_a_name: str, col_b_name: str,\n",
        "             title: str=None, norm: bool=False, subtitle: str=\"\",\n",
        "             rolling: int=None, corr: bool=True, figsize: Tuple[int, int]=(9,6)\n",
        "             ) -> None:\n",
        "    \"\"\"\n",
        "    Convenience function to plot two columns with dates (aka time-series) and\n",
        "    possibly correlation information.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    # Get the minimum and maximum dates\n",
        "    mindate, maxdate = df['date'].min(),df['date'].max()\n",
        "    # Make the x-axis \"tight\" (no empty space before or after the actual data)\n",
        "    ax.set_xlim(mindate,maxdate)\n",
        "    # Make the mininum and maximum date \"pretty\"\n",
        "    mindate_str, maxdate_str = mindate.strftime('%b %d, %Y'), maxdate.strftime('%b %d, %Y')\n",
        "    # Get human readable column name and units\n",
        "    name_a, units_a = human_col_info(col_a_name)\n",
        "    name_b, units_b = human_col_info(col_b_name)\n",
        "    # If the units are different, use two axes so that scale is (likely) less\n",
        "    # of an issue.\n",
        "    if not norm and units_a != units_b:\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.set_ylabel(units_b)\n",
        "    else:\n",
        "        ax2 = ax\n",
        "        pass\n",
        "    # Create a temporary DataFrame so that we can change things if needed\n",
        "    # without affecting the original DataFrame\n",
        "    tmp_df = df[[col_a_name, col_b_name, 'date']]\n",
        "    # Normalize the data, if we're supposed to.\n",
        "    if norm:\n",
        "        tmp_df[col_a_name] = normalize_to_mean(tmp_df[col_a_name])\n",
        "        tmp_df[col_b_name] = normalize_to_mean(tmp_df[col_b_name])\n",
        "        pass\n",
        "    # Use a rolling mean if we're supposed to.\n",
        "    if rolling:\n",
        "        tmp_df[col_a_name] = tmp_df[col_a_name].rolling(rolling).mean()\n",
        "        tmp_df[col_b_name] = tmp_df[col_b_name].rolling(rolling).mean()\n",
        "        pass\n",
        "    # Plot the lines\n",
        "    tmp_df.plot(x='date', y=col_a_name, ax=ax, lw=1.5, alpha=0.5, color=colors[0], legend=False)\n",
        "    tmp_df.plot(x='date', y=col_b_name, ax=ax2, lw=1.5, alpha=0.5, color=colors[1], legend=False)\n",
        "    # Create a temp DataFrame with only the two columns we just plotted and\n",
        "    # remove any rows witn NaNs in them (the stats package Spearman/Pearson\n",
        "    # functions don't like NaNs, but the DataFrame versions don't return a\n",
        "    # p-value, so we're going to use the stats version and need to remove\n",
        "    # all the NaNs)\n",
        "    tmp_df.dropna(inplace=True)\n",
        "    # Get to Numpy arrays for the stats package. Technically, not needed,\n",
        "    # but pandas Series / DataFrames sometimes throw warnings, so this way\n",
        "    # we don't see them.\n",
        "    col_a, col_b = tmp_df[col_a_name].to_numpy(), tmp_df[col_b_name].to_numpy()\n",
        "    # Need to do a custom label because we (may) have two axes\n",
        "    patch_a = mpatches.Patch(color=colors[0], alpha=0.5, label=f\"{name_a} ({units_a})\")\n",
        "    patch_b = mpatches.Patch(color=colors[1], alpha=0.5, label=f\"{name_b} ({units_b})\")\n",
        "    ax.legend(handles=[patch_a, patch_b])\n",
        "    # Add Spearman and Pearson Correlation Coefficient values if we're supposed\n",
        "    # to\n",
        "    if corr:\n",
        "        spearman_r, spearman_p = stats.spearmanr(col_a, col_b)\n",
        "        pearson_r, pearson_p = stats.pearsonr(tmp_df[col_a_name], tmp_df[col_b_name])\n",
        "        subtitle = f\"Pearson R = {pearson_r:0.3f}, p = {printable_p(pearson_p)};\"\n",
        "        subtitle += f\" Spearman R = {spearman_r:0.3f}, p = {printable_p(spearman_p)}\\n\"\n",
        "        subtitle += f\"From {mindate_str} to {maxdate_str}\"\n",
        "        pass\n",
        "    if rolling:\n",
        "        subtitle += f\", {rolling}-Day Rolling Mean.\"\n",
        "    if norm:\n",
        "        subtitle += \"\\nValues Normalized to mean of 0.0 and Standard Dev of 1.0\"\n",
        "        ax.set_ylabel(\"Values Normalized to Mean 0.0, Std Dev 1.0\")\n",
        "    else:\n",
        "        if ax == ax2:\n",
        "            ax.set_ylabel(units_a)\n",
        "        else:\n",
        "            ax.set_ylabel(units_a)\n",
        "            ax2.set_ylabel(units_b)\n",
        "            pass\n",
        "        pass\n",
        "    # If we were not provided a title, make one up:\n",
        "    if title is None:\n",
        "        title = f\"{name_a} ({units_a}) vs. {name_b} ({units_b}) in Philadelphia\"\n",
        "        pass\n",
        "    # Append the subtitle to the title. Getting the title and subtitle to look\n",
        "    # nice but with different font sizes turned out to be a pain, so I gave up\n",
        "    # on that.\n",
        "    title += \"\\n\" + subtitle\n",
        "    ax.set_title(title, fontsize=14)\n",
        "    # Add the x-values label\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    # Make the plot \"tight\" because I think it looks better.\n",
        "    plt.tight_layout()\n",
        "    return\n",
        "\n",
        "\n",
        "def plot_cols(df: pd.DataFrame, cols: list, title: str=None,\n",
        "              norm: bool=False, subtitle: str=\"\", figsize: Tuple[int, int]=(9,6)\n",
        "              ) -> None:\n",
        "    \"\"\"\n",
        "    Convenience function to plot one or more columns with dates.\n",
        "    \"\"\"\n",
        "    # Make a copy so we don't clobber any data\n",
        "    df = df.copy()\n",
        "    # If it's just two, call the plot_two_ts() function in stead.\n",
        "    if len(cols) == 2:\n",
        "        return plot_two_ts(df, cols[0], cols[1], title, norm, subtitle)\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    # Get the minimum and maximum dates\n",
        "    mindate, maxdate = df['date'].min(),df['date'].max()\n",
        "    # Make the x-axis \"tight\" (no empty space before or after the actual data)\n",
        "    ax.set_xlim(mindate,maxdate)\n",
        "    # Make the mininum and maximum date \"pretty\"\n",
        "    mindate_str, maxdate_str = mindate.strftime('%b %d, %Y'), maxdate.strftime('%b %d, %Y')\n",
        "    labels = []\n",
        "    for col in cols:\n",
        "        if 'date' == col:\n",
        "            continue\n",
        "        name, units = human_col_info(col)\n",
        "        label = f\"{name} ({units})\"\n",
        "        labels.append(label)\n",
        "        if norm:\n",
        "            df[col] = normalize_to_mean(df[col])\n",
        "            pass\n",
        "        df.plot(x='date', y=col, ax=ax, lw=1.5, alpha=0.5)\n",
        "        pass\n",
        "    ax.legend(labels=labels)\n",
        "    if norm:\n",
        "        subtitle += \"Normalized to mean of 0.0 and Standard Dev of 1.0\"\n",
        "        ax.set_ylabel(\"Values Normalized to Mean 0.0, Std Dev 1.0\")\n",
        "        pass\n",
        "    title += \"\\n\" + subtitle\n",
        "    ax.set_title(title, fontsize=14)\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    plt.tight_layout()\n",
        "    return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjI0HfSYeGRm"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fob6iodEiqgc"
      },
      "source": [
        "## Data Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amnquuU5iu_s"
      },
      "source": [
        "col_units = {\n",
        "    'pm25_ugm3': r'${{\\mu}g}/{m^3}$',\n",
        "    'co_ppm': r'$ppm$',\n",
        "    'o3_ppm': r'$ppm$',\n",
        "    'no2_ppb': r'$ppb$',\n",
        "    'so2_ppb': r'$ppb$',\n",
        "    'temp_f_max': r'$°F$',\n",
        "    'temp_f_mean': r'$°F$',\n",
        "    'temp_f_min': r'$°F$',\n",
        "    'dew_f_max': r'$°F$',\n",
        "    'dew_f_mean': r'$°F$',\n",
        "    'dew_f_min': r'$°F$',\n",
        "    'humid_max': r'$Percent$',\n",
        "    'humid_mean': r'$Percent$',\n",
        "    'humid_min': r'$Percent$',\n",
        "    'wind_mph_max': r'$mph$',\n",
        "    'wind_mph_mean': r'$mph$',\n",
        "    'wind_mph_min': r'$mph$',\n",
        "    'mmhg_max': r'$mmHg$',\n",
        "    'mmhg_mean': r'$mmHg$',\n",
        "    'mmhg_min': r'$mmHg$',\n",
        "    'precip_inches': r'$inches$',\n",
        "    'temp_c_max': r'$°C$',\n",
        "    'temp_c_mean': r'$°C$',\n",
        "    'temp_c_min': r'$°C$',\n",
        "    'dew_c_max': r'$°C$',\n",
        "    'dew_c_mean': r'$°C$',\n",
        "    'dew_c_min': r'$°C$',\n",
        "    'wind_kph_max': r'$km/h$',\n",
        "    'wind_kph_mean': r'$km/h$',\n",
        "    'wind_kph_min': r'$km/h$',\n",
        "    'precip_cm': r'$cm$',\n",
        "    'temp_dew_diff_c': r'$°C$',\n",
        "    'covid_hosp': r'$Number$',\n",
        "    'day_of_week': f'$0-6$',\n",
        "}\n",
        "\n",
        "col_names = {\n",
        "    'pm25_ugm3': r'Particulate Matter ≥ 2.5 $nm$',\n",
        "    'co_ppm': r'Carbon Monoxide',\n",
        "    'o3_ppm': 'Ozone',\n",
        "    'no2_ppb': 'Nitrogen Dioxide',\n",
        "    'so2_ppb': 'Sulfur Dioxide',\n",
        "    'temp_f_max': r'Max Temperature',\n",
        "    'temp_f_mean': r'Mean Temperature',\n",
        "    'temp_f_min': r'Min Temperature',\n",
        "    'dew_f_max': r'Max Dew Point',\n",
        "    'dew_f_mean': r'Mean Dew Point',\n",
        "    'dew_f_min': r'Min Dew Point',\n",
        "    'humid_max': r'Max Relative Humidity',\n",
        "    'humid_mean': r'Mean Relative Humidity',\n",
        "    'humid_min': r'Min Relative Humidity',\n",
        "    'wind_mph_max': r'Max Wind Speed',\n",
        "    'wind_mph_mean': r'Mean Wind Speed',\n",
        "    'wind_mph_min': r'Min Wind Speed',\n",
        "    'mmhg_max': r'Max Atmospheric Pressure',\n",
        "    'mmhg_mean': r'Mean Atmospheric Pressure',\n",
        "    'mmhg_min': r'Min Atmospheric Pressure',\n",
        "    'precip_inches': r'Precipitation',\n",
        "    'temp_c_max': r'Max Temperature',\n",
        "    'temp_c_mean': r'Mean Temperature',\n",
        "    'temp_c_min': r'Min Temperature',\n",
        "    'dew_c_max': r'Max Dew Point',\n",
        "    'dew_c_mean': r'Mean Dew Point',\n",
        "    'dew_c_min': r'Min Dew Point',\n",
        "    'wind_kph_max': r'Max Wind Speed',\n",
        "    'wind_kph_mean': r'Mean Wind Speed',\n",
        "    'wind_kph_min': r'Min Wind Speed',\n",
        "    'precip_cm': r'Precipitation',\n",
        "    'temp_dew_diff_c': r'Mean Temerature Minus Mean Dew Point',\n",
        "    'covid_hosp': r'COVID-19 Related Hospitalizations',\n",
        "    'day_of_week': r'Day of Week as a Number',\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9fLqfTBuA_d"
      },
      "source": [
        "## Daily Weather Data\n",
        "The daily weather information was manually scraped from https://www.wunderground.com/history/daily/us/pa/philadelphia/KPHL/date/ on 2021-02-10 and stored in Google Drive as a CSV. For some reason there are two days which are missing any data from this website."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7bs012cYO7o"
      },
      "source": [
        "# \"Download\" the weather data from the Google Sheet containing all the data for this project.\n",
        "weather_df = get_google_sheet_df(\"1F5DWdR3tAGRSqoPfFwnrvXiZV_WHCbdfv3T_uGFjsiY\",\"Philly Weather 2020\")\n",
        "weather_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-Bp1RYaUoi"
      },
      "source": [
        "# Make sure the date column is a datetime\n",
        "weather_df['date'] = pd.to_datetime(weather_df['date'], infer_datetime_format=True, errors='ignore')\n",
        "# Include only data for 2019 or 2020\n",
        "indexes = (weather_df['date']>=pd.Timestamp(2019,1,1)) & (weather_df['date']<pd.Timestamp(2021,1,1))\n",
        "weather_df = weather_df[indexes]\n",
        "weather_df['date_idx'] = weather_df['date']\n",
        "weather_df.set_index('date_idx', inplace=True)\n",
        "# Show information of the data we've downloaded.\n",
        "weather_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JglePLZTubX1"
      },
      "source": [
        "# Add celsius data\n",
        "for col in weather_df.columns:\n",
        "    if \"_f_\" not in col:\n",
        "        continue\n",
        "    first, second = col.split(\"_f_\")\n",
        "    new_col = f\"{first}_c_{second}\"\n",
        "    weather_df[new_col] = (weather_df[col] - 32) * (5/9)\n",
        "    pass\n",
        "# Add km/h data\n",
        "for col in weather_df.columns:\n",
        "    if \"_mph_\" not in col:\n",
        "        continue\n",
        "    first, second = col.split(\"_mph_\")\n",
        "    new_col = f\"{first}_kph_{second}\"\n",
        "    weather_df[new_col] = weather_df[col] * 1.60934\n",
        "    pass\n",
        "weather_df['precip_cm'] = weather_df['precip_inches'] * 2.54"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0CjHZMGMZoU"
      },
      "source": [
        "## Air Quality Data\n",
        "\n",
        "Downloaded from https://www.epa.gov/outdoor-air-quality-data/download-daily-data manually converted to one aggregated CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sup2GYZCMV6S"
      },
      "source": [
        "aq_df = get_google_sheet_df(\"1F5DWdR3tAGRSqoPfFwnrvXiZV_WHCbdfv3T_uGFjsiY\",\"Philly AQ All 2020\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlXd-d6CQSb6"
      },
      "source": [
        "aq_df.set_index('date', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytSDmOn41yiu"
      },
      "source": [
        "# aqs_param  units   \n",
        "# PM2.5      ug/m3 LC    503\n",
        "# O3         ppm         346\n",
        "# CO         ppm         275\n",
        "# NO2        ppb         274\n",
        "# S02        ppb         270\n",
        "def get_specific_df(df, aqs_param, new_col_name):\n",
        "    \"\"\"\n",
        "    Create a DataFrame for a specific air quality measurement where there\n",
        "    is zero or one reading per day by taking the average (mean) for those\n",
        "    days with more than one reading.\n",
        "    \"\"\"\n",
        "    # Get only the values we want.\n",
        "    df = df[(aq_df['aqs_param'] == aqs_param)]\n",
        "    # Only the one column, but keep it as a DataFrame not a Series\n",
        "    df = df['value'].to_frame().dropna()\n",
        "    # Rename the column\n",
        "    df.rename(columns={'value':new_col_name}, inplace=True)\n",
        "    # Remove non-numbers\n",
        "    df.dropna(inplace=True)\n",
        "    # Group by date and take the mean. Basically, if there is more than\n",
        "    # one reading for that day, take the mean of all readings and just\n",
        "    # keep the mean. This way, all days have one and only one \"reading\"\n",
        "    df = df.groupby('date').mean()\n",
        "    # Return this new DataFrame\n",
        "    return df\n",
        "\n",
        "def create_merged_air_quality_df(df):\n",
        "    \"\"\"\n",
        "    Take the aq_df and create a df with columns that are the individual readings\n",
        "    for an air quality measure and the rows are one row per day within the\n",
        "    time frame included.\n",
        "    \"\"\"\n",
        "    # Create a DataFrame with one PM2.5 reading per day which is an average of all\n",
        "    # readings for that day\n",
        "    pm25_df = get_specific_df(aq_df, 'PM2.5', 'pm25_ugm3')\n",
        "    # Repeate for other readings\n",
        "    co_df = get_specific_df(aq_df,'CO','co_ppm')\n",
        "    o3_df = get_specific_df(aq_df,'O3','o3_ppm')\n",
        "    no2_df = get_specific_df(aq_df,'NO2','no2_ppb')\n",
        "    so2_df = get_specific_df(aq_df,'SO2','so2_ppb')\n",
        "    # Merge them all together using the date index as the key\n",
        "    aq2_df = pd.merge(pm25_df,co_df, how='outer', left_index=True, right_index=True)\n",
        "    aq2_df = pd.merge(aq2_df,o3_df, how='outer', left_index=True, right_index=True)\n",
        "    aq2_df = pd.merge(aq2_df,no2_df, how='outer', left_index=True, right_index=True)\n",
        "    aq2_df = pd.merge(aq2_df,so2_df, how='outer', left_index=True, right_index=True)\n",
        "    aq2_df.sort_index()\n",
        "    aq2_df['date'] = pd.to_datetime(aq2_df.index)\n",
        "    return aq2_df\n",
        "\n",
        "aq2_df = create_merged_air_quality_df(aq_df)\n",
        "aq2_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsnauUQkgy3i"
      },
      "source": [
        "## Merge the weather data with the air quality data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Glt8ouSB_t1s"
      },
      "source": [
        "# Merge the above into the add_df DataFrame so we can do some easy plotting\n",
        "# and analysis.\n",
        "all_df = pd.merge(aq2_df, weather_df, how='outer', left_index=True, right_index=True).sort_index()\n",
        "all_df['date'] = pd.to_datetime(all_df.index, infer_datetime_format=True, errors='ignore')\n",
        "all_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L19S97sYVSDc"
      },
      "source": [
        "## COVID-19 Related Data\n",
        "\n",
        "Downloaded from https://phl.carto.com/api/v2/sql?filename=covid_hospitalizations_by_date&format=csv&skipfields=cartodb_id,the_geom,the_geom_webmercator&q=SELECT%20*%20FROM%20covid_hospitalizations_by_date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMCAXJLPVLuZ"
      },
      "source": [
        "covid_df = get_google_sheet_df(\"1F5DWdR3tAGRSqoPfFwnrvXiZV_WHCbdfv3T_uGFjsiY\",'COVID-19 Hospitalizations')\n",
        "covid_df = covid_df[['date','count']].groupby('date').sum()\n",
        "covid_df.columns = ['covid_hosp']\n",
        "covid_df.dropna(inplace=True)\n",
        "covid_df['date'] = pd.to_datetime(covid_df.index, infer_datetime_format=True, errors='ignore')\n",
        "covid_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDvQ85cIWA8n"
      },
      "source": [
        "plot_cols(covid_df, ['covid_hosp'], \"COVID-19 Replated Hospitalizations\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxtLTHJeY6Rp"
      },
      "source": [
        "# The previous chart looked suspiciously like there might be a weekly pattern to reporting, so let's look at day of week information.\n",
        "\n",
        "covid_df['day_of_week'] = covid_df['date'].dt.dayofweek\n",
        "plot_two_ts(covid_df, 'covid_hosp', 'day_of_week', figsize=(19,12)) #, title=\"Number of Reported COVID-19 Hospitalizations vs Day of Week\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REycxQigcZO9"
      },
      "source": [
        "# The previous chart looked suspiciously like there might be a weekly pattern to reporting, so let's look at day of week information.\n",
        "\n",
        "covid_df['day_of_week'] = covid_df['date'].dt.dayofweek\n",
        "\n",
        "mean = covid_df.groupby('day_of_week')['covid_hosp'].mean()\n",
        "std = covid_df.groupby('day_of_week')['covid_hosp'].std()\n",
        "fig, ax = plt.subplots(figsize=(9,6))\n",
        "ax.errorbar(mean.index, mean, xerr=0.5, yerr=2*std, linestyle='')\n",
        "ax.set_title(\"Mean Number of Reported Hospitalizations with 95% CI\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFyrxAhmANZG"
      },
      "source": [
        "# Let's see if we can factor out the weekday effect.\n",
        "covid_df['covid_hosp_rolling_mean'] = covid_df['covid_hosp'].rolling(7).mean()\n",
        "col_names['covid_hosp_rolling_mean'] = 'COVID-19 Reported Hosp. 7-Day Rolling Mean'\n",
        "col_units['covid_hosp_rolling_mean'] = '$Number$'\n",
        "covid_df['covid_hosp_rolling_diff'] = covid_df['covid_hosp'] - covid_df['covid_hosp_rolling_mean']\n",
        "col_names['covid_hosp_rolling_diff'] = 'COVID-19 Reported Hosp. Minus 7-Day Rolling Mean'\n",
        "col_units['covid_hosp_rolling_diff'] = '$Number$'\n",
        "covid_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D4oMo-HbAjp"
      },
      "source": [
        "all_df = pd.merge(all_df,covid_df, how='outer', left_index=True, right_index=True)\n",
        "all_df['date'] = pd.to_datetime(all_df.index, errors='ignore', infer_datetime_format=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um1arqqeC1dt"
      },
      "source": [
        "plot_two_ts(covid_df, 'covid_hosp_rolling_diff', 'day_of_week', title=\"Number of Reported COVID-19 Hospitalizations vs Day of Week\\nAfter Subtracting Rolling Mean\", figsize=(18,12))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jzxpHTvCp1j"
      },
      "source": [
        "mean = covid_df.groupby('day_of_week')['covid_hosp_rolling_diff'].mean()\n",
        "std = covid_df.groupby('day_of_week')['covid_hosp_rolling_diff'].std()\n",
        "fig, ax = plt.subplots(figsize=(9,6))\n",
        "ax.errorbar(mean.index, mean, xerr=0.5, yerr=2*std, linestyle='')\n",
        "ax.set_title(\"Mean Number of Reported Hospitalizations with 95% CI\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S_jz9rOA9cH"
      },
      "source": [
        "# Exploratory Data Anaylsis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHzdHUsNvG8x"
      },
      "source": [
        "## Check weather correlations\n",
        "Just testing out some known correlations as a sanity check.\n",
        "\n",
        "Here we plot the minimum and maximum temperatures over time. They should\n",
        "look like they follow each other rather closely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTAtlckphxHz"
      },
      "source": [
        "plot_two_ts(all_df, 'temp_c_max', 'temp_c_min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zLUEp7D6kVU"
      },
      "source": [
        "Now we try the same for the dewpoint which should also track pretty closely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edu31tzXhjBq"
      },
      "source": [
        "plot_two_ts(all_df, 'dew_c_max', 'dew_c_min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF0xkb-V6slS"
      },
      "source": [
        "Lastly, let's check the mean temerature and mean dew point (which should track closely) along with the amount of precipitation, whcich may or may not be correlated with the above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCdVuoXxhwHK"
      },
      "source": [
        "plot_two_ts(all_df, 'dew_c_mean', 'temp_c_mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBrDHbHW-EBL"
      },
      "source": [
        "plot_two_ts(all_df, 'dew_c_mean', 'precip_cm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWBJHk4YmxGt"
      },
      "source": [
        "# dew point minus min temperature\n",
        "all_df['temp_dew_diff_c'] = all_df['temp_c_mean'] - all_df['dew_c_mean']\n",
        "col_names['temp_dew_diff_c'] = 'Mean Temp Minus Mean Dew Pt'\n",
        "col_units['temp_dew_diff_c'] = '$°C$'\n",
        "plot_two_ts(all_df, 'temp_dew_diff_c', 'precip_cm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3pA5Suu-7fc"
      },
      "source": [
        "plot_two_ts(all_df[all_df['temp_c_min'] > 0.0], 'temp_c_mean', 'precip_cm', \"Above Freezing Temperatures vs. Precipitation in Philadelphia\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhOmXVuv-a1N"
      },
      "source": [
        "plot_two_ts(all_df, 'temp_c_mean', 'wind_kph_mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7Vjxq1MkIdZ"
      },
      "source": [
        "## Paring the Data Set Down to 2020 Only\n",
        "\n",
        "For now, I am going to only look at 2020 data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY8_Zq5GkP4U"
      },
      "source": [
        "indexes = (all_df['date']>=pd.Timestamp(2020,1,1)) & (all_df['date']<pd.Timestamp(2021,1,1))\n",
        "all_df=all_df[indexes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOz9rBWLAzND"
      },
      "source": [
        "## Air Quality Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QEjSAj4bpfQ"
      },
      "source": [
        "plot_cols(aq2_df, [col for col in aq2_df.columns if 'date' != col],\n",
        "          f\"All Air Quality Measures in Philadelphia in Data Set\",\n",
        "          norm=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSD91dH1cCY5"
      },
      "source": [
        "plot_two_ts(aq2_df, 'pm25_ugm3','co_ppm', norm=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8sDHdMXcMoB"
      },
      "source": [
        "plot_two_ts(aq2_df, 'pm25_ugm3','o3_ppm', norm=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQnNV-oJeQ3y"
      },
      "source": [
        "plot_two_ts(aq2_df, 'pm25_ugm3','no2_ppb', norm=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDMuVTYseAnu"
      },
      "source": [
        "plot_two_ts(aq2_df, 'co_ppm','o3_ppm', norm=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT3kV-xAfKWo"
      },
      "source": [
        "plot_two_ts(aq2_df, 'co_ppm','no2_ppb', norm=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCTzymHaCV95"
      },
      "source": [
        "plot_two_ts(aq2_df, 'co_ppm','no2_ppb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI691DlSfRRe"
      },
      "source": [
        "plot_two_ts(aq2_df, 'o3_ppm','no2_ppb', norm=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3aCQuOmg6ek"
      },
      "source": [
        "## Calculate multiple cross correlations.\n",
        "\n",
        "I chose to use the [Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) in stead of the more common [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) because I assumed that any correlations may not be strictly linear and the Wikipedia page on Spearman suggested that it might be more robust to non-linear correlations than the Pearson without increasing the liklihood of [spurious correlations](https://tylervigen.com/spurious-correlations)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltC8IbSgg0Sz"
      },
      "source": [
        "### Everything v Everything\n",
        "\n",
        "I first wanted to see a cross-correlation of everything to everything, both for curiosity and also a sanity check. I am initially only interested in the magnitude of the correlation (e.g., the absolute value), since I think that very negatively correlated columns are just as interesting as very positively correlated items.\n",
        "\n",
        "I use the pandas built-in corr [function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html) which does this for me and then used the [seabrone heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html) method to display. I didn't find the default heatmap colors easy to see so I picked a different colormap.\n",
        "\n",
        "In order to exclude \"self-correlations\", I set all correlation values of 1.0 to [numpy.nan](https://numpy.org/doc/stable/reference/constants.html) which eliminates them from the heatmap.\n",
        "\n",
        "Luckily, even the converted columns had correlations of 1.0, so it all looked good."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u8x3_CMJmij"
      },
      "source": [
        "# Get the cross-correlation of all columns. I chose the Spearman rank order\n",
        "# correlation coefficient instead of the default Pearson because the Spearman\n",
        "# is less sensitive to non-linear correlations without significantly increasing\n",
        "# false-positives and I believe that some of the correlations of interest may\n",
        "# be non-linear. See Also:\n",
        "# https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\n",
        "corrs = all_df.corr(method='spearman')\n",
        "fig, ax = plt.subplots(figsize=(18,12))\n",
        "sns.heatmap(corrs.replace(1.0, np.nan).abs(), annot=True, ax=ax, cmap=\"YlGnBu\")\n",
        "ax.set_title(\"Absolute Value of Spearman Rank Order Correlation Coefficients $≤ 1.0$\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdiEZWGwiwoJ"
      },
      "source": [
        "### Most Things v Most Things\n",
        "\n",
        "Since I really want to look at things which may have correlations but are not completely obvious (e.g., not min temperature vs. max temperature), and I don't need to cross correlate coverted columns, I remove all of the empirical columns and keep only the metric columns. After going over the values, I noted that all correlations above 0.8 were uninteresting (e.g., like the min and max temperature), so I want to plot only values ≤ 0.8. This still leaves a couple non-interesting correlations (e.g., min and max humidity or min and max dew points) but it leaves some interesting ones which are surprisingly strongly correlated (e.g., temperature and COVID-19 hospitalizations)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gQTE3HVLu8A"
      },
      "source": [
        "emperical_columns = []\n",
        "for x in all_df.columns:\n",
        "    if 'inch' in x or 'mph' in x or '_f_' in x:\n",
        "        emperical_columns.append(x)\n",
        "        pass\n",
        "    pass\n",
        "tmp_df = all_df.drop(columns=emperical_columns + ['so2_ppb']).corr(method='spearman')\n",
        "labels = []\n",
        "mapper = {}\n",
        "max_corr = 0.8\n",
        "for col in tmp_df.columns:\n",
        "    name, units = human_col_info(col)\n",
        "    units = units.replace('$','').replace('{','').replace('}','').replace('\\\\','')\n",
        "    label = f\"{name} ({units})\"\n",
        "    # print(f\"col -> '{label}'\")\n",
        "    labels.append(label)\n",
        "    mapper[col] = label\n",
        "    pass\n",
        "tmp_df.columns = labels\n",
        "tmp_df.rename(mapper, inplace=True)\n",
        "fig, ax = plt.subplots(figsize=(18,12))\n",
        "tmp_df[tmp_df.abs() > max_corr] = np.nan\n",
        "sns.heatmap(tmp_df.abs(), annot=True, ax=ax, cmap=\"YlGnBu\")\n",
        "ax.set_title(f\"Absolute Value of Spearman Rank Order Correlation Coefficients $≤ {max_corr}$\")\n",
        "# ax.set_xlabel(labels)\n",
        "# ax.set_ylabel(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOrR8_u_3ja4"
      },
      "source": [
        "### Looking Deeper at Temperature v COVID-19 Hospitalizations\n",
        "\n",
        "We found a strong negative correlation (Spearman R = $-0.770$, p = $6.1\\times10^{-58}$; Pearson R = $-0.753$, p = $3.7\\times10^{-54}$) between temperature and the 7-day rolling mean for the reported number of COVID-19 related hospitalizations. This is somewhat consistent with published findings (Xie 2020).\n",
        "\n",
        "**Citations**\n",
        "\n",
        "Xie, J., & Zhu, Y. (2020). Association between ambient temperature and COVID-19 infection in 122 cities from China. Science of the Total Environment, 724, 138201. [link](https://doi.org/10.1016/j.scitotenv.2020.138201)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGG68-a8e_GK"
      },
      "source": [
        "plot_two_ts(all_df, 'temp_c_mean', 'covid_hosp_rolling_mean', figsize=(18,12))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmeUP5oh4532"
      },
      "source": [
        "#### Contradicting other findings.\n",
        "\n",
        "However, Zoran et al. (Zoran 2020) found a positive correlation between ground-level ozone and COVID-19 infections, wereas I found a modest negative correlation (Spearman R = $-0.395$, p = $1.5\\times10^{-11}$, Pearson R = $-0.452$, p = $4.7\\times10^{-15}$).\n",
        "\n",
        "Similarly, I found that $NO_2$ was positively correlated with COVID-19 hospitalizations (Spearman R = $-0.131$, p = $0.06$ [not significant], Pearson R = $-0.299$, p = $1.8\\times10^{-5}$), which is the opposite of what was reported by Zoran et al. I was also unable to validate (not significant) the negative relationship with humidity or precipitation while again I found the opposite correlation with wind (Spearman R = $-0.202$, p = $5.4\\times10^{-4}$, Pearson R = $-0.210$, p = $3.2\\times10^{-4}$).\n",
        "\n",
        "**Citations**\n",
        "\n",
        "Zoran, M. A., Savastru, R. S., Savastru, D. M., & Tautan, M. N. (2020). Assessing the relationship between ground levels of ozone (O3) and nitrogen dioxide (NO2) with coronavirus (COVID-19) in Milan, Italy. Science of The Total Environment, 740, 140005. [link](https://doi.org/10.1016/j.scitotenv.2020.140005)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0MQHKqH5yFq"
      },
      "source": [
        "plot_two_ts(all_df, 'o3_ppm', 'covid_hosp_rolling_mean')\n",
        "plot_two_ts(all_df, 'no2_ppb', 'covid_hosp_rolling_mean')\n",
        "plot_two_ts(all_df, 'humid_mean', 'covid_hosp_rolling_mean')\n",
        "plot_two_ts(all_df, 'wind_kph_mean', 'covid_hosp_rolling_mean')\n",
        "plot_two_ts(all_df, 'precip_cm', 'covid_hosp_rolling_mean')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek0BmXtYJs3A"
      },
      "source": [
        "# List the things that most correlate with particulate matter in the air.\n",
        "# We use the absolute value because we're only interested in the magnitude\n",
        "# and we want only values below 0.9 because the other pm25 columnes have\n",
        "# correlations above 0.9 but nothing else does.\n",
        "corrs['pm25_ugm3'].abs()[corrs['pm25_ugm3'].abs() < 0.9].sort_values()[-10:]\n",
        "# From the above output, we see that values most correlated with particulate\n",
        "# matter in the air are the average (mean) wind speeds and the minimum recorded\n",
        "# wind speed, which makes sense since mininum wind speed is more strongly\n",
        "# correlated with average wind speed than maximum wind speed (think gusts)\n",
        "# interestingly, precipitation seems to have almost no correlation with\n",
        "# particulate matter in the air, which I did not expect, even though\n",
        "# humidity had a moderate correlation."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPhyx7GcNn5K"
      },
      "source": [
        "# One of the issues is that the definition of \"mean\" wind speed was not well\n",
        "# defined at the source. The wind_mph_mean and wind_kmh_mean columns are clearly\n",
        "# not the means of the min and max, but I'm not sure how they are calculates\n",
        "# (e.g., sampled each hour? minute? second? then over the day's 24 hours?)\n",
        "# and I worried that the \"spikiness\" (e.g., variance) of the data might make\n",
        "# detecting a \"real\" correlation more challenging. Assuming that on most days\n",
        "# the real particulate matter content across the entier greater Philadelphia\n",
        "# area varies less bruskly than once-or-twice-a-day readings at one or two\n",
        "# locations (the readings are irregular), I believe it is appropriate to consider\n",
        "# a 7-day smoothed average to better represent the \"real\" particulate matter\n",
        "# air content for the area, so I rolled up the values to 7-day averages here:\n",
        "rolling_df = all_df.rolling(7).mean()\n",
        "rolling_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_8k9k_kQGT8"
      },
      "source": [
        "# Now I recalculate the Spearman cross-corralations on the moving average data\n",
        "rolling_corrs = rolling_df.corr(method='spearman')\n",
        "rolling_corrs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8owoBKNJbr1k"
      },
      "source": [
        "rolling_df['date'] = pd.to_datetime(rolling_df.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMIxqO4xIq2O"
      },
      "source": [
        "### Air Borne Particulate Matter v Wind Speed\n",
        "\n",
        "\n",
        "\n",
        "**Citations**\n",
        "\n",
        "Jones, A. M., Harrison, R. M., & Baker, J. (2010). The wind speed dependence of the concentrations of airborne particulate matter and NOx. Atmospheric Environment, 44(13), 1682-1690.\n",
        "\n",
        "Zhang, B., Jiao, L., Xu, G., Zhao, S., Tang, X., Zhou, Y., & Gong, C. (2018). Influences of wind and precipitation on different-sized particulate matter concentrations (PM 2.5, PM 10, PM 2.5–10). Meteorology and Atmospheric Physics, 130(3), 383-392."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_6Ug2LNM-w9"
      },
      "source": [
        "plot_two_ts(all_df, 'pm25_ugm3','wind_kph_mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us0y6D5ei1X4"
      },
      "source": [
        "plot_two_ts(all_df, 'pm25_ugm3','wind_kph_mean', rolling=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5XwI2jmQXXz"
      },
      "source": [
        "# And repeat for the PM2.5 column as before to find the largest magnitude\n",
        "# correlations:\n",
        "for col in \"pm25_ugm3\tco_ppm\to3_ppm\tno2_ppb\".split():\n",
        "    print(f\"---- Spearman R Values with {col}\")\n",
        "    print(rolling_corrs[col].abs()[rolling_corrs[col].abs() < 0.999].sort_values()[-10:])\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s-h72J0GcVA"
      },
      "source": [
        "### Ozone v Temp Known Correlations\n",
        "Here I check if I see what has been generally established - namely that ozone increases with temperature. I do find reasonably strong correlations (Pearson R = 0.66, Spearman R = 0.69) that are highly significant (p << 0.00000000001)\n",
        "\n",
        "**Note:** It is surprising to me that the ozone levels seem to anticipate the rise in temperature on the rolling-mean plot.\n",
        "\n",
        "**Citations:**\n",
        "\n",
        "Barnett, J. J., Houghton, J. T., & Pyle, J. A. (1975). The temperature dependence of the ozone concentration near the stratopause. Quarterly Journal of the Royal Meteorological Society, 101(428), 245-257.\n",
        "\n",
        "Olszyna, K. J., Luria, M., & Meagher, J. F. (1997). The correlation of temperature and rural ozone levels in southeastern USA. Atmospheric environment, 31(18), 3011-3022.\n",
        "\n",
        "Pusede, S. E., Steiner, A. L., & Cohen, R. C. (2015). Temperature and recent trends in the chemistry of continental surface ozone. Chemical reviews, 115(10), 3898-3918."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6x2WdJ1BVFB"
      },
      "source": [
        "plot_two_ts(rolling_df, 'o3_ppm','temp_c_mean', rolling=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGElev4oHikh"
      },
      "source": [
        "### NO2 v Temp\n",
        "\n",
        "Likewise, I check to see if NO2 is inversely correlated with temperature as established in the literature. The inverse correlations exist (Pearson R = -0.79, Spearman R = -0.79) both highly significant.\n",
        "\n",
        "**Citations**\n",
        "\n",
        "Schindlbacher, A., Zechmeister‐Boltenstern, S., & Butterbach‐Bahl, K. (2004). Effects of soil moisture and temperature on NO, NO2, and N2O emissions from European forest soils. Journal of Geophysical Research: Atmospheres, 109(D17).\n",
        "\n",
        "Goldberg, D. L., Anenberg, S. C., Kerr, G. H., Mohegh, A., Lu, Z., & Streets, D. G. (2021). TROPOMI NO2 in the United States: A detailed look at the annual averages, weekly cycles, effects of temperature, and correlation with surface NO2 concentrations. Earth's Future, e2020EF001665."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THQCmfoUhz-L"
      },
      "source": [
        "plot_two_ts(rolling_df, 'no2_ppb','temp_c_mean', rolling=7)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}